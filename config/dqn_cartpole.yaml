# Configuraci√≥n de DQN
# input_shape: (4,)
# num_actions: 2
trainer:
  type: 'DQNTrainer'
environment:
  name: "CartPole-v1"
  max_episode_steps: 4000
  render_mode: 'rgb_array'
  # reward_threshold: 995
model:
  type: "MPLModel"
  dense_layers: 
    - units: 256
      activation: "relu"
      kernel_initializer: "he_uniform"
    - units: 128
      activation: "relu"
      kernel_initializer: "he_uniform"
    - units: 64
      activation: "relu"
      kernel_initializer: "he_uniform"
policy:
  type: "epsilon"
  num_actions: 2
  epsilon: 1.0
  epsilon-min: 0.001
  epsilon-decay: 0.999
replay-buffer:
  type: "standard"
  capacity: 2000
agent:
  type: 'DQNAgent'
  episodes: 1000
  gamma: 0.95
  batch-size: 64
  train-start: 1500
metrics:
  - class: "MeanAbsoluteError"
    params:
      name: "mae"
  - class: "RootMeanSquaredError"
    params:
      name: "root_mse"
  - class: "MeanAbsolutePercentageError"
    params:
      name: "mape"
  - class: "R2Score"
    params:
      name: "r2_score"
graphs:
  - title: Score
    figsize: [20, 15]
    xlabel: "Episodes"
    ylabel: "Values"
    plots:
      - data: "loss"
        label: "Perdida"
      - data: "mae"
        label: "Mean absolute error"
      - data: "root_mse"
        label: "Root mean squared error"
      - data: "mape"
        label: "Mean absolute percentage error"
      - data: "r2_score"
        label: "R^2 score"
